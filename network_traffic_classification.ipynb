{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41c95531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\\n%matplotlib inline\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\\n%matplotlib inline\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258274b4",
   "metadata": {},
   "source": [
    "# Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edc5751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset preprocessing\n",
    "import pandas as pd\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245c69ec",
   "metadata": {},
   "source": [
    "# NSL-KDD Dataset Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d268b4c3",
   "metadata": {},
   "source": [
    "The NSL-KDD dataset was collated in order to mitigate the problems of the KDD'99 dataset. The KDD'99 dataset attracted the attention of many researchers due to the lack of public data sets for signature based intrusion detection systems (IDSs). Tavallaee et al. [1] found that the most significant issue with the 1999 dataset is that it contained a significant number of redundant instances such that classifiers were biased towards these instances while disregarding infrequent instances. However, it is often these infrequent instances that can be indicative of new devastating network intrusions.\n",
    "\n",
    "Some of the improvements present in the NSL-KDD dataset is that all redundant instances were removed. Moreover, the number of records from each difficulty group was selected to be inversely proportional to the percentage of records in the original KDD dataset. Difficulty refers to how many of 21 classifiers were able to classify an instance correctly from the 1999 dataset. In addition, some of the instances in the original dataset contained unsystematic noise, thus these instances were discarded when constructing the improved dataset. Overall, the original KDD dataset was disproportionately distributed, thus rendering the classifiers ineffective in serving as a discriminative tool for detecting network intrusions that can be identified by signature.\n",
    "\n",
    "The NSL-KDD dataset can be downloaded from: https://www.unb.ca/cic/datasets/nsl.html\n",
    "\n",
    "Please note that a 20% subset of the whole dataset will be used due to the magnitude of the whole dataset (more than 120,000 instances). The 20% subset is also provided at the above link under the name 'KDDTrain+_20Percent.TXT'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474d61be",
   "metadata": {},
   "source": [
    "# Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c27268",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"ids.csv\")\n",
    "print(f\"Number of Instances: {df.shape[0]}\")\n",
    "print(f\"Number of Features (including class): {df.shape[1]}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8bc3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_missing_values = df.isnull().sum().sum()\n",
    "print(\"There are %d missing values in the dataset.\" % (num_of_missing_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c04f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = df.dtypes.value_counts().to_frame().reset_index()\n",
    "data_types.columns = [\"Type\", \"Count\"]\n",
    "data_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94acea74",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = df.select_dtypes(include=[\"object\"])\n",
    "categorical_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e75a1d",
   "metadata": {},
   "source": [
    "## Feature Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf3c2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faa9d05",
   "metadata": {},
   "source": [
    "The majority of the continuous features such as duration, src_bytes, and dst_bytes have very large standard deviations and ranges. Thus, these continuous features will be transformed to a feature range of [0, 1] in the preprocessing phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3bb6ff",
   "metadata": {},
   "source": [
    "## Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d974ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df[\"class\"].value_counts().index\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "_, _, autopcts = ax.pie(\n",
    "    df[\"class\"].value_counts(),\n",
    "    textprops={\"fontsize\": 14},\n",
    "    labels=labels,\n",
    "    colors=(\"green\", \"red\"),\n",
    "    autopct=\"%.2f%%\",\n",
    "    startangle=60,\n",
    "    explode=(0, 0.05),\n",
    ")\n",
    "\n",
    "plt.setp(autopcts, **{\"color\": \"white\", \"weight\": \"bold\", \"fontsize\": 13})\n",
    "ax.set_title(\"Class Distribution\", fontdict={\"fontsize\": 16})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de83ffbe",
   "metadata": {},
   "source": [
    "Overall, the class distribution is slightly skewed towards the 'Normal' class, thus stratified cross validation (CV) will be used to ensure that the folds preserve the percentage of samples for each class. The 'Normal' class represents network traffic that is not indicative of an intrusion while the 'Anomaly' class represents network traffic that is associated with malicious behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a367eee",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980040e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [col for col in list(df) if df[col].nunique() <= 1]\n",
    "df = df.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823f224f",
   "metadata": {},
   "source": [
    "All features with only 1 unique value are dropped since they are futile for a classification task. The only 2 features that have 1 unique value are 'num_outbound_cmds' and 'is_host_login'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04548db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1]\n",
    "y = df[\"class\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15293131",
   "metadata": {},
   "source": [
    "The data is separated into two components. X contains the features of the dataset while y contains the class labels for each instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47899a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_features = df.columns[df.isin([0, 1]).all()].tolist()\n",
    "binary_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440d85d1",
   "metadata": {},
   "source": [
    "All the binary features do not need to be scaled to a feature range of [0, 1], thus the feature names are extracted from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff6e1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features_nb = df.select_dtypes(\"number\").columns.drop(binary_features).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5f7300",
   "metadata": {},
   "source": [
    "All the numeric features that are non-binary, i.e., continuous, will be scaled to a feature range of [0, 1] using MinMaxScaler()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d721fdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OrdinalEncoder(handle_unknown=\"ignore\")\n",
    "mms = MinMaxScaler()\n",
    "\n",
    "ct = make_column_transformer(\n",
    "    (enc, [\"protocol_type\", \"service\", \"flag\"]),\n",
    "    (mms, numeric_features_nb),\n",
    "    remainder=\"passthrough\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4d7c59",
   "metadata": {},
   "source": [
    "The 3 categorical features, 'protocol_type', 'service', and 'flag' are encoded using an ordinal encoder. The ordinal encoder transforms a categorical feature to an integer range in 0 to n_categories - 1. The above 3 feature seem to be nominal, thus I initially tried using one hot encoding. However, after using the ordinal encoder, I compared the accuracy of the models and found that all classifiers achieved higher accuracy using the ordinal encoder. Furthermore, the one hot encoder resulted in the 'service' feature being split into 66 distinct columns as that feature can take on 66 values. The higher accuracy of the classifiers using ordinal encoding on the categorical features demonstrates that although these feature may seem to be nominal based on the concepts they represent, they are highly likely to posses a natural ordering. The ordinal encoder is set to handle unknown feature values from the testing set during CV evaluation by ignoring such values. Thus, sklearn's ordinal encoder was used as opposed to the pandas ordinal encoder since it can handle unknown categorical features seen during CV.\n",
    "\n",
    "The class labels did not need to be encoded since the majority of sklearn's classifiers automatically encode the target labels using a label encoder. All other features are already numeric, thus they were not encoded via the 'remainder=\"passthrough\"' setting.\n",
    "\n",
    "All these feature transformations (column transformations) will be used in a pipeline. A pipeline was used as it makes it easier to compose estimators since at every fit or predict call within the CV procedure, it will automatically apply the column transformations. This will prevent information leakage since the feature transformations will not be applied to the training set as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d1dc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_names(ct):\n",
    "    ct_temp = ct\n",
    "    ct_temp.fit(X, y)\n",
    "    features = []\n",
    "\n",
    "    # disregard remainder = \"passthrough\"\n",
    "    for transformer in ct_temp.transformers_[:-1]:\n",
    "        features += transformer[2]\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "all_features = extract_feature_names(ct)\n",
    "\n",
    "# have to define passedthrough_features since they were not explicitly stated\n",
    "# when creating the column transformer\n",
    "passedthrough_features = [\n",
    "    \"land\",\n",
    "    \"urgent\",\n",
    "    \"logged_in\",\n",
    "    \"root_shell\",\n",
    "    \"num_shells\",\n",
    "    \"is_guest_login\",\n",
    "]\n",
    "all_features = all_features + passedthrough_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634cacdc",
   "metadata": {},
   "source": [
    "The feature names are extracted from the column transformer since the features will be re-ordered by the transformer. This will be used to extract the feature names when feature selection is used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
